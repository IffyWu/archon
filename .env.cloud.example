# ==========================================
# Archon 云服务器部署环境变量配置
# ==========================================

# === SUPABASE 配置 ===
# 重要：必须使用 SERVICE ROLE 密钥，不要使用 ANON 密钥！
# SERVICE ROLE 密钥通常以 eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI 开头
# ANON 密钥权限不足会导致后端服务启动失败
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key-here  # ⚠️ 必须是 SERVICE ROLE 密钥
SUPABASE_ANON_KEY=your-anon-key-here  # 可选，前端使用

# === HOST 配置（云服务器必需）===
# 设置为您的云服务器公网 IP 或域名
# 不要使用 localhost，否则容器间无法通信
HOST=your-server-ip-or-domain  # 例如: 123.456.789.0 或 archon.yourdomain.com

# === 服务端口配置 ===
# 如果云服务器上有端口冲突，可以修改这些端口
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181  
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838

# === LLM 提供商配置 ===
# 选择您的 LLM 提供商
LLM_PROVIDER=openai  # 可选: openai, google, ollama

# OpenAI 配置
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini  # 或 gpt-4, gpt-3.5-turbo

# Google Gemini 配置（可选）
# GOOGLE_API_KEY=your-google-api-key-here
# GOOGLE_MODEL=gemini-2.0-flash-exp

# Ollama 配置（可选，适合本地部署）
# OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
# OLLAMA_MODEL=llama3.2

# === 嵌入模型配置 ===
EMBEDDING_MODEL=text-embedding-3-small
CONTEXTUAL_EMBEDDINGS_MAX_WORKERS=3

# === RAG 配置 ===
USE_CONTEXTUAL_EMBEDDINGS=true
USE_HYBRID_SEARCH=true
USE_AGENTIC_RAG=true
USE_RERANKING=false

# === 性能优化（云服务器）===
# 根据服务器配置调整
CRAWL_BATCH_SIZE=10  # 降低以减少内存使用
CRAWL_MAX_CONCURRENT=3  # 降低并发数
DOCUMENT_STORAGE_BATCH_SIZE=10
EMBEDDING_BATCH_SIZE=5

# === 日志配置 ===
LOG_LEVEL=INFO  # 可选: DEBUG, INFO, WARNING, ERROR
PYDANTIC_LOGFIRE_TOKEN=  # 可选，用于高级日志分析

# === 安全配置 ===
# 在生产环境中建议启用
SECURE_COOKIES=true
CORS_ORIGINS=["http://${HOST}:${ARCHON_UI_PORT}"]

# === Docker 网络配置 ===
# 通常不需要修改
DOCKER_NETWORK=archon_default

# ==========================================
# 部署检查清单：
# 1. ✅ 使用 SERVICE ROLE 密钥（不是 ANON 密钥）
# 2. ✅ 设置正确的 HOST（云服务器 IP 或域名）
# 3. ✅ 配置 LLM API 密钥
# 4. ✅ 确保端口未被占用
# 5. ✅ 服务器有足够的内存（建议 4GB+）
# ==========================================